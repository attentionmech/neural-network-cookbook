# neural-network-excercises

| #  | Task                                                                                          | File Path       |
|----|-----------------------------------------------------------------------------------------------|-----------------|
| 1  | Build a simple perceptron to solve the AND gate problem.(bare)                                       | [src/nn1.py](src/nn1.py)    |
| 2  | Train the perceptron to solve the OR gate problem.(bare)                                            | [src/nn2.py](src/nn2.py)    |
| 3  | Build a small MLP with one hidden layer to solve the XOR problem.(bare)                              | [src/nn3.py](src/nn3.py)    |
| 4  | Visualize the decision boundary of a trained perceptron or MLP on the XOR problem.            | [src/nn4.py](src/nn4.py)    |
| 5  | Train an MLP with one hidden layer on a custom 2D dataset (e.g., points inside/outside a circle). | [src/nn5.py](src/nn5.py)    |
| 6  | Add a second hidden layer to the MLP and train it on the XOR problem.                          | [src/nn6.py](src/nn6.py)    |
| 7  | Implement a learning rate schedule (e.g., decrease learning rate during training).            | [src/nn7.py](src/nn7.py)    |
| 8  | Add L2 regularization to an MLP to reduce overfitting.                                         | [src/nn8.py](src/nn8.py)    |
| 9  | Visualize Sigmoid activation in a MLP.                                                         | [src/nn9.py](src/nn9.py)    |
| 10 | Use dropout regularization on an MLP and observe its effect on training.                       | [src/nn10.py](src/nn10.py)  |
| 11 | Train a neural network with an Adam optimizer instead of SGD on a binary classification task. | [src/nn11.py](src/nn11.py)  |
| 12 | Split a dataset into training and validation sets and track overfitting.                      | [src/nn12.py](src/nn12.py)  |
| 13 | Implement early stopping during training to prevent overfitting.                              | [src/nn13.py](src/nn13.py)  |
| 14 | Train a neural network for multi-class classification (e.g., 3 or more classes).              | [src/nn14.py](src/nn14.py)  |
| 15 | Normalize inputs (e.g., using MinMaxScaler or StandardScaler) before training.               | [src/nn15.py](src/nn15.py)  |
| 16 | Build an autoencoder to learn an encoding for simple data (e.g., a small 2D dataset).         | [src/nn16.py](src/nn16.py)  |
| 17 | Train an autoencoder and visualize the reconstruction of the input data.                      | [src/nn17.py](src/nn17.py)  |
| 18 | Implement batch normalization and observe its effect on training speed and performance. | [src/nn18.py](src/nn18.py) |
| 19 | Create a simple Convolutional Neural Network (CNN) for MNIST digit classification. | [src/nn19.py](src/nn19.py) |
| 20 | Implement transfer learning by fine-tuning a pre-trained model on a small dataset. | [src/nn20.py](src/nn20.py) |
| 21 | Build a Recurrent Neural Network (RNN) for a simple sequence prediction task. | [src/nn21.py](src/nn21.py) |
| 22 | Implement a Long Short-Term Memory (LSTM) network for time series prediction. | [src/nn22.py](src/nn22.py) |
| 23 | Create a Generative Adversarial Network (GAN) for generating simple 2D point distributions. | [src/nn23.py](src/nn23.py) |
| 24 | Implement weight initialization strategies (Xavier/Glorot, He initialization). | [src/nn24.py](src/nn24.py) |
| 25 | Build a neural network ensemble and compare its performance to individual models. | [src/nn25.py](src/nn25.py) |
| 26 | Create a custom loss function and apply it to a neural network. | [src/nn26.py](src/nn26.py) |
| 27 | Implement k-fold cross-validation for model evaluation. | [src/nn27.py](src/nn27.py) |
| 28 | Design a neural network for function approximation (e.g., approximating sin(x)). | [src/nn28.py](src/nn28.py) |
| 29 | Create a simple attention mechanism in a neural network. | [src/nn29.py](src/nn29.py) |
| 30 | Implement a siamese neural network for similarity learning. | [src/nn30.py](src/nn30.py) |
| 31 | Build a neural network to classify synthetic dataset with non-linear decision boundaries. | [src/nn31.py](src/nn31.py) |
| 32 | Create a neural network that can handle missing data through masking. | [src/nn32.py](src/nn32.py) |
| 33 | Implement a variational autoencoder (VAE) for data generation. | [src/nn33.py](src/nn33.py) |
| 34 | Design a neural network that can explain its predictions using saliency maps. | [src/nn34.py](src/nn34.py) |
| 35 | Create a neural network for dimensionality reduction (similar to PCA). | [src/nn35.py](src/nn35.py) |
